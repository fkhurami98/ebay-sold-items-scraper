import csv
from datetime import datetime
import os
import random
import re
from bs4 import BeautifulSoup
import time
import requests
import urllib.parse
from fake_useragent import UserAgent

search_keywords: list= ["Lego 75280", "Lego 75309", "Lego 75342"]
max_pages: int = 1
items_per_page: int = 120       # 60, 120, 240


def get_number_of_search_results(soup):
    # Find the correct <script> tag - adjust the criteria as needed for your specific case
    script_tag = soup.find("script", text=re.compile(r'"text":"\d+ results"'))
    if script_tag:
        script_content = script_tag.string  # Get the string content of the <script> tag
        # This pattern looks for the number of results in the format '"text":"<number> results"'
        results_pattern = re.compile(r'"text":"(\d+) results"')
        match = results_pattern.search(script_content)
        if match:
            # If found, return the number of results as an integer
            return int(match.group(1))
    # If the script tag with the number of results isn't found, return None
    return None


def extract_data(soup):
    listings = []
    for listing in soup.select(".s-item__wrapper"):
        item_condition = (
            listing.select_one(".s-item__subtitle .SECONDARY_INFO").text.strip()
            if listing.select_one(".s-item__subtitle .SECONDARY_INFO")
            else "Not specified"
        )
        listing_type = (
            listing.select_one(".s-item__detail .BOLD").text.strip()
            if listing.select_one(".s-item__detail .BOLD")
            else (
                listing.select_one(".s-item__bids.s-item__bidCount").text.strip()
                if listing.select_one(".s-item__bids.s-item__bidCount")
                else (
                    listing.select_one(".s-item__purchase-options").text.strip()
                    if listing.select_one(".s-item__purchase-options")
                    else "Not specified"
                )
            )
        )
        shipping_detail = (
            listing.select_one(".s-item__logisticsCost").text.strip()
            if listing.select_one(".s-item__logisticsCost")
            else "Shipping details not specified"
        )
        price = (
            listing.select_one(".s-item__price").text.strip()
            if listing.select_one(".s-item__price")
            else "Price not specified"
        )
        seller_info = (
            listing.select_one(".s-item__seller-info-text").text.strip()
            if listing.select_one(".s-item__seller-info-text")
            else "Seller ID not specified"
        )
        sold_date = (
            listing.select_one(".s-item__title--tag .POSITIVE").text.strip()
            if listing.select_one(".s-item__title--tag .POSITIVE")
            else "Sold date not specified"
        )
        title = (
            listing.select_one(".s-item__image img")["alt"].strip()
            if listing.select_one(".s-item__image img")
            else "Title not specified"
        )
        url = (
            listing.select_one(".s-item__link")["href"].strip()
            if listing.select_one(".s-item__link")
            else "URL not specified"
        )
        listings.append(
            {
                "title": title,
                "url": url,
                "seller_info": seller_info,
                "price": price,
                "shipping_detail": shipping_detail,
                "listing_type": listing_type,
                "item_condition": item_condition,
                "sold_date": sold_date,
            }
        )
    return listings


def save_to_csv(data, filename):
    with open(filename, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(
            [
                "Title",
                "Price",
                "URL",
                "Seller Info",
                "Shipping Detail",
                "Listing Type",
                "Item Condition",
                "Sold Date",
            ]
        )
        for item in data:
            writer.writerow(
                [
                    item["title"],
                    item["price"],
                    item["url"],
                    item["seller_info"],
                    item["shipping_detail"],
                    item["listing_type"],
                    item["item_condition"],
                    item["sold_date"],
                ]
            )


if __name__ == "__main__":
    user_agent = UserAgent()

    for keyword in search_keywords:
        date = datetime.now().strftime("%Y-%m-%d")
        filename = f"ebay_data_{keyword.replace(' ', '_')}_{date}.csv"

        for page_number in range(1, max_pages + 1):
            encoded_keyword = urllib.parse.quote_plus(keyword)
            base_url = f"https://www.ebay.co.uk/sch/i.html?_from=R40&_nkw={encoded_keyword}&_sacat=0&LH_Sold=1&LH_Complete=1&LH_ItemCondition=1000&rt=nc&_pgn={page_number}&_ipg={items_per_page}"
            headers = {"User-Agent": user_agent.random}

            response = requests.get(url=base_url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            number_of_searches = get_number_of_search_results(soup)
            print(number_of_searches)

            # Save the HTML content to a file
            html_filename = os.path.join(
                f"{keyword.replace(' ', '_')}_{page_number}.html"
            )
            with open(html_filename, "w", encoding="utf-8") as file:
                # Use prettify() to format the HTML content if needed
                file.write(soup.prettify())

            listings = extract_data(soup)
            save_to_csv(listings, filename)
            print(f"Data from page {page_number} for {keyword} saved to '{filename}'.")

            time.sleep(random.uniform(1, 10))
